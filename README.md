# 🎮 中国象棋 AI 训练系统

<div align="center">

基于 **AlphaZero** 算法的中国象棋 AI，通过自我对弈不断进步

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-orange.svg)](https://pytorch.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

[功能特性](#-功能特性) • [快速开始](#-快速开始) • [使用指南](#-使用指南) • [版本历史](#-版本历史)

</div>

---

## 📸 项目展示

### 训练进度实时监控
```
【第 5/100 轮训练】
时间: 2025-11-27 14:50:00

>> 阶段1: 自我对弈 (100局)...
   当前训练局数: 1159, MCTS模拟次数: 15
   使用4进程并行对弈...
   进度: [==========================                ] 52/100 (52.0%) | 有效:52 (100%)

   [完成] 100 局对弈
   统计: 红方2胜 黑方1胜 97和
   [保存] 3 局精彩对局 (有胜负:3, 总计:3)

>> 阶段2: 训练神经网络...
   平均损失: 0.0098 | 学习率: 0.001000
```

### 训练进度可视化
运行 `python plot_progress.py` 查看训练曲线：
- 📈 胜率变化趋势
- 📊 平均步数统计
- 🎯 累计对局数
- 🥧 胜负分布饼图

### 人机对战界面
运行 `python main.py play` 与 AI 对战：
- 🎨 精美的图形界面
- 🖱️ 鼠标点击操作
- 💡 合法走法提示（绿色圆圈）
- ⚡ 实时 AI 思考过程

### 精彩对局回放
运行 `python view_best_games.py` 查看精彩对局：
- 🎬 自动播放对局过程
- ⏸️ 暂停/继续/单步控制
- ⚡ 可调节播放速度（1-5倍）
- 📊 显示对局信息（时间、步数、结果）

---

## ✨ 功能特性

### 🎯 核心功能
- ✅ **完整的象棋规则引擎** - 支持所有传统中国象棋规则
- ✅ **AlphaZero 算法** - 基于深度强化学习的自我对弈训练
- ✅ **MCTS 搜索** - 蒙特卡洛树搜索，智能决策
- ✅ **神经网络训练** - 价值网络和策略网络

### 🚀 性能优化
- ⚡ **多进程加速** - 4 核并行训练，速度提升 4 倍
- 📊 **动态参数调整** - MCTS 模拟次数和学习率自动优化
- 🎯 **激进奖励机制** - 快速学习吃子和将军策略
- 💾 **智能缓存** - 将帅位置缓存，减少重复计算

### 🎮 用户体验
- 📈 **实时进度条** - 可视化训练进度（百分比 + 有效率）
- 🎬 **精彩对局回放** - 自动保存并回放有胜负的对局
- 📊 **训练曲线图** - 胜率、步数等多维度统计
- 🖥️ **图形界面** - 支持人机对战和观看对局

### 🛡️ 稳定性
- 💾 **自动保存** - 训练过程自动保存模型
- ⚡ **优雅中断** - Ctrl+C 安全退出，数据不丢失
- 🔄 **断点续训** - 重启后自动加载上次模型继续训练
- 🐛 **完善的错误处理** - 多进程异常捕获和恢复

---

## 🚀 快速开始

### 环境要求
- Python 3.8+
- PyTorch 2.0+
- CUDA 支持（推荐，GPU 训练快 10 倍）

### 安装依赖
```bash
# 克隆项目
git clone https://github.com/hpy666666/ChineseChessAI.git
cd ChineseChessAI

# 安装依赖
pip install torch pygame numpy matplotlib
```

### 一键开始训练
```bash
python main.py train
```

### 快速验收成果
```bash
# 1. 快速评估（10局测试）
python main.py evaluate

# 2. 人机对战
python main.py play

# 3. 观看对局
python main.py watch

# 4. 查看训练曲线
python plot_progress.py

# 5. 回放精彩对局
python view_best_games.py
```

---

## 📖 使用指南

### 所有可用命令

| 命令 | 功能 | 说明 |
|------|------|------|
| `python main.py train` | 🎓 训练模式 | 自我对弈100局/轮，约7.5分钟 |
| `python main.py play` | 🎮 人机对战 | 与AI对战，验收成果 ⭐ |
| `python main.py evaluate` | 📊 快速评估 | 10局测试，显示实力等级 ⭐ |
| `python main.py watch` | 👀 观看对局 | 图形界面，速度可调 |
| `python plot_progress.py` | 📈 训练曲线 | 胜率、步数趋势图 |
| `python view_best_games.py` | 🎬 精彩回放 | 查看有胜负的对局 ⭐ |
| `python main.py help` | ❓ 帮助 | 查看所有命令 |

### 训练参数配置

编辑 `config.py` 调整训练参数：

```python
# 训练速度 vs 质量权衡
SELF_PLAY_GAMES = 100      # 每轮对弈局数
MAX_MOVES = 70             # 每局最大步数（引导快速结束）
NUM_WORKERS = 4            # 并行进程数（=CPU核心数）

# MCTS参数（自动调整）
# - 初期（0-1000局）: 15次模拟
# - 中期（1000-8000局）: 25-35次
# - 后期（8000+局）: 45-50次

# 学习率（自动调整）
# - 初期（0-5000局）: 0.001
# - 中期（5000-15000局）: 0.0005
# - 后期（15000+局）: 0.0002
```

### 训练进度时间线

| 训练局数 | 预期表现 | 实力等级 | 所需时间* |
|---------|---------|---------|----------|
| 100 | 随机走棋 | 完全随机 | 10分钟 |
| 500 | 知道规则 | 初识规则 | 1小时 |
| 1,000 | 开始吃子 | 入门级 | 2小时 |
| 2,000 | 偶尔胜负 | 业余初级 | 4小时 |
| 5,000 | 30%有胜负 | 业余初级+ | 10小时 |
| 10,000 | 50%有胜负 | 业余中级 | 20小时 |
| 20,000+ | 稳定对局 | 业余高级 | 40小时+ |

*基于 RTX 4070 + 4核并行的速度估算

---

## 🎯 验收成果的 5 种方式

### 1️⃣ 快速评估 - 量化指标
```bash
python main.py evaluate
```
显示：
- ✅ 红方/黑方/和局胜率
- ✅ 平均步数统计
- ✅ 预估实力等级
- ✅ 训练局数和进度

### 2️⃣ 人机对战 - 直观体验
```bash
python main.py play
```
特点：
- 🖱️ 鼠标点击操作
- 💡 绿色圆圈显示合法走法
- ⚡ 可选红方/黑方
- 📊 显示 AI 实力等级

### 3️⃣ 观看对局 - 了解思路
```bash
python main.py watch
```
功能：
- 👀 观看 AI 自我对弈
- ⚡ 可调节播放速度
- 📊 显示候选走法数量
- 🎮 空格暂停/继续

### 4️⃣ 训练曲线 - 长期趋势
```bash
python plot_progress.py
```
包含：
- 📈 胜率变化曲线（红/黑/和）
- 📊 平均步数趋势
- 🎯 累计对局数
- 🥧 胜负分布饼图

### 5️⃣ 精彩回放 - 高光时刻
```bash
# 查看所有精彩对局
python view_best_games.py

# 播放第3局
python view_best_games.py --index 3

# 播放最近5局
python view_best_games.py --latest 5
```
控制：
- ⏸️ 空格：暂停/继续
- ⬅️➡️ 方向键：上一步/下一步
- 1️⃣-5️⃣ 数字键：调整速度
- ❌ Q键：退出

---

## 🏗️ 项目结构

```
ChineseChessAI/
├── 📄 核心代码
│   ├── chess_env.py          # 象棋规则引擎
│   ├── neural_network.py     # 神经网络模型
│   ├── self_play.py          # MCTS自我对弈
│   └── trainer.py            # 训练管理器
│
├── 🎮 用户工具
│   ├── main.py               # 统一入口
│   ├── visualizer.py         # 图形界面（人机对战+观看）
│   ├── evaluate.py           # 快速评估
│   ├── plot_progress.py      # 训练曲线
│   ├── view_best_games.py    # 精彩对局回放 ⭐
│   └── compare_models.py     # 模型对比
│
├── ⚙️ 配置
│   ├── config.py             # 训练参数配置
│   └── requirements.txt      # 依赖列表
│
├── 📁 数据目录
│   ├── data/                 # 对局数据
│   │   └── best_games.pkl    # 精彩对局存档 ⭐
│   ├── models/               # 模型文件
│   │   └── latest.pt         # 最新模型
│   └── logs/                 # 训练日志
│       ├── training.log      # 训练记录
│       └── evaluation_history.txt  # 评估历史
│
└── 📚 文档
    ├── README.md             # 项目说明（本文件）
    ├── 今日总结_*.md         # 开发日志
    └── docs_backup/          # 历史文档
```

---

## 🎨 性能优化亮点

### 训练速度提升 4.5 倍
| 优化项 | 原始 | 优化后 | 提升 |
|--------|------|--------|------|
| 环境复制 | 深拷贝 | 状态保存恢复 | 2倍 |
| MCTS模拟 | 固定50次 | 动态15-50次 | 25%-29% |
| 步数限制 | 100步 | 70步 | 30% |
| 多进程 | 单进程 | 4核并行 | 4倍 |
| **综合提升** | 100秒/局 | **52秒/局** | **48%** |

### 智能动态参数

**MCTS 模拟次数自动调整：**
- 0-1,000 局: 15 次（快速探索）
- 1,000-3,000 局: 25 次（逐步加强）
- 3,000-8,000 局: 35 次（中等质量）
- 8,000+ 局: 50 次（完整思考）

**学习率自动衰减：**
- 0-5,000 局: 0.001（快速学习）
- 5,000-15,000 局: 0.0005（稳定提升）
- 15,000+ 局: 0.0002（精细调优）

---

## 🔧 高级功能

### 精彩对局自动保存 ⭐
训练过程中自动保存：
- ✅ 所有有胜负的对局（红胜/黑胜）
- ✅ 步数少于50步的快速对局
- 📁 保存位置：`data/best_games.pkl`
- 💾 最多保留：500局精彩对局

### 实时训练监控 ⭐
```
进度: [====================              ] 40/100 (40.0%) | 有效:40 (100%)
```
显示：
- 📊 实时进度百分比
- ✅ 有效对局数量
- 📈 完成速度预估

### 多进程训练优化
- 🚀 支持 4 核并行对弈
- ⚡ 优雅的 Ctrl+C 中断
- 💾 中断时自动保存已完成数据
- 🔄 断点续训无缝衔接

---

## 🐛 常见问题

### Q1: 训练需要多久才能看到效果？
**A:** 根据新的激进奖励机制：
- 1,000-1,500 局：开始出现吃子行为
- 1,500-2,000 局：出现第一次胜负
- 3,000-5,000 局：约 30% 对局有胜负

推荐：先训练到 2,000 局（约 4 小时）再评估。

### Q2: 如何查看 AI 进步情况？
**A:** 5 种方式：
1. `python main.py evaluate` - 快速评估
2. `python plot_progress.py` - 查看训练曲线
3. `python main.py play` - 人机对战
4. `python view_best_games.py` - 查看精彩对局
5. 查看 `logs/training.log` - 详细日志

### Q3: 可以暂停训练吗？
**A:** 可以！按 `Ctrl+C` 即可：
- ✅ 自动保存当前模型
- ✅ 已完成的对局数据不会丢失
- ✅ 下次运行自动加载继续训练

### Q4: 训练速度慢怎么办？
**A:** 检查：
1. 是否启用了 GPU？（运行时会显示 "使用设备: cuda"）
2. `config.py` 中 `USE_MULTIPROCESSING = True`？
3. `NUM_WORKERS` 设置为 CPU 核心数（如 4）？

### Q5: 为什么全是和棋？
**A:** 训练初期（0-1500局）AI 还不会进攻，全是和棋很正常。
最新优化已大幅加速学习：
- ✅ 吃子奖励放大 20 倍
- ✅ 将军奖励放大 15 倍
- ✅ 和局有轻微惩罚
- ✅ 步数限制降至 70 步

预计 1,500-2,000 局开始出现胜负。

---

## 📊 版本历史

### v2.0.0 - 重大优化更新 (2025-11-27) ⭐

**核心改进：**
- 🎯 激进奖励机制（吃子奖励×20，将军奖励×15）
- 📈 动态训练参数（MCTS、学习率自动调整）
- 🚀 训练速度提升 50%（每局从100秒降至52秒）
- 📊 实时进度条（百分比+有效率显示）

**新增功能：**
- 🎬 精彩对局自动保存和回放器（`view_best_games.py`）
- 📉 学习率实时显示
- ⚡ 评估默认局数优化（20→10局）

**Bug修复：**
- 🐛 修复 `visualizer.py` 缺少 `os` 导入
- 🐛 修复 `evaluate.py` 的 `black_rate` 参数错误
- 🐛 修复 `plot_progress.py` 格式化和除零错误
- 🐛 修复 `main.py` 的 `sys.argv` 冲突

**预期效果：**
- ⚡ 训练速度提升约 50%
- 🎯 预计 1,000-2,000 局出现胜负（原需 5,000+局）
- 🎮 用户体验大幅改善

---

### v1.0.0 - 初始版本 (2025-11-26)

**基础功能：**
- ✅ 完整的中国象棋规则引擎
- ✅ AlphaZero 算法实现
- ✅ MCTS 搜索树
- ✅ 神经网络训练

**性能优化：**
- ⚡ 多进程并行训练（4核）
- 💾 将帅位置缓存
- 🔄 优化环境复制
- 📊 批量神经网络推理

**用户工具：**
- 🎮 人机对战模式
- 👀 观看AI对局
- 📊 快速评估
- 📈 训练曲线图
- 🔄 模型对比

**稳定性：**
- 💾 自动保存/加载模型
- ⚡ Ctrl+C 优雅中断
- 🔄 断点续训
- 🐛 多进程异常处理

---

## 🤝 贡献指南

欢迎提交 Issue 和 Pull Request！

### 开发环境设置
```bash
git clone https://github.com/hpy666666/ChineseChessAI.git
cd ChineseChessAI
pip install -r requirements.txt
```

### 运行测试
```bash
python main.py test
```

---

## 📄 许可证

本项目采用 [MIT License](LICENSE)

---

## 🙏 致谢

- **AlphaZero** - 算法灵感来源
- **PyTorch** - 深度学习框架
- **Pygame** - 图形界面支持

---

## 📮 联系方式

- GitHub: [@hpy666666](https://github.com/hpy666666)
- 项目链接: [ChineseChessAI](https://github.com/hpy666666/ChineseChessAI)

---

<div align="center">

**如果这个项目对你有帮助，请给个 ⭐ Star 支持一下！**

Made with ❤️ and ☕

</div>
