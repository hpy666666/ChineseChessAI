# 今日总结 - 2025-11-27

## 🎯 主要任务

修复多进程训练模式下的 Ctrl+C 退出问题

---

## 🐛 遇到的问题

### 问题1: Intel MKL 库报错
**现象:**
```
forrtl: error (200): program aborting due to control-C event
Image              PC                Routine            Line        Source
KERNELBASE.dll     00007FFDEDB9C24D  Unknown               Unknown  Unknown
```

**原因:** PyTorch 使用的 Intel MKL (Math Kernel Library) 在 Windows 多进程环境下对 Ctrl+C 处理不当

### 问题2: CUDA 多进程共享错误
**现象:**
```
RuntimeError: CUDA error: invalid device context
```

**原因:** GPU 张量在多进程间传递时，Ctrl+C 导致 CUDA 上下文混乱

### 问题3: 训练局数不增加
**现象:** 按 Ctrl+C 中断后，已完成的对局数据丢失，`total_games` 计数器不增加

**原因:** KeyboardInterrupt 异常导致数据处理代码未执行，训练阶段被跳过

---

## ✅ 解决方案

### 方案 A: 禁用 Intel MKL 信号处理

**修改文件:** `main.py`, `self_play.py`

**实现:**
```python
# main.py (启动时设置)
os.environ["FOR_DISABLE_CONSOLE_CTRL_HANDLER"] = "1"

# self_play.py (子进程中设置)
def _play_game_worker(args):
    import os
    os.environ["FOR_DISABLE_CONSOLE_CTRL_HANDLER"] = "1"
    signal.signal(signal.SIGINT, signal.SIG_IGN)  # 子进程忽略信号
```

**效果:** 消除了 `forrtl: error (200)` 错误

---

### 方案 B: 修复 CUDA 多进程问题

**修改文件:** `self_play.py`

**问题根源:**
```python
# 错误：直接传递 GPU 张量
network_state_dict = network.state_dict()  # 在 GPU 上
```

**解决方法:**
```python
# 正确：先转到 CPU 再传递
network_state_dict = {k: v.cpu() for k, v in network.state_dict().items()}
```

**效果:** 消除了 CUDA 上下文错误

---

### 方案 C: 保存中断时的数据

**核心思路:** 像单进程模式一样，用已完成的数据训练，更新局数，保存模型

**实现步骤:**

#### 1. 创建自定义异常类（self_play.py）
```python
class InterruptedWithResults(Exception):
    """自定义异常：中断时携带已完成的结果"""
    def __init__(self, results):
        self.results = results
        super().__init__("Training interrupted by user")
```

#### 2. 在 parallel_self_play 中抛出（self_play.py）
```python
except KeyboardInterrupt:
    print("\n\n⚠️  检测到Ctrl+C，正在终止所有子进程...")
    if pool:
        pool.terminate()
        pool.join(timeout=10)
    print("✓ 已停止对弈")
    print(f"提示: 已完成 {len(results)} 局对弈，数据将被保存")

    # 抛出自定义异常，携带已完成的结果
    raise InterruptedWithResults(results)
```

#### 3. 在 collect_self_play_data 中捕获（trainer.py）
```python
results = []
interrupted = False
try:
    results = parallel_self_play(...)
except InterruptedWithResults as e:
    # 捕获自定义异常，获取已完成的结果
    print("\n对弈被中断")
    results = e.results
    interrupted = True

# 处理已完成的数据（无论正常完成还是中断）
for game_data, winner in results:
    self.replay_buffer.push(game_data)
    self.total_games += 1  # ← 关键：局数会增加！
    total_moves += len(game_data)
    ...

# 处理完后再抛出异常
if interrupted:
    raise KeyboardInterrupt
```

#### 4. 在 train_loop 中处理（trainer.py）
```python
for iteration in range(1, num_iterations + 1):
    try:
        # 1. 对弈
        stats = self.collect_self_play_data(SELF_PLAY_GAMES)

        # 2. 训练
        if len(self.replay_buffer) >= BATCH_SIZE:
            avg_loss = self.train_network()

        # 3. 保存
        if iteration % SAVE_INTERVAL == 0:
            self.save_model()

    except KeyboardInterrupt:
        # 中断时：先用已收集的数据训练，再保存，最后退出
        print("\n>> 训练被中断，处理已收集的数据...")

        if len(self.replay_buffer) >= BATCH_SIZE:
            avg_loss = self.train_network()  # 用已有数据训练

        self.save_model()  # 保存模型
        print(f"   总对局数: {self.total_games}")

        raise  # 让外层知道训练结束
```

---

## 📊 最终效果

### 按 Ctrl+C 的完整流程

**完成了 10/100 局时按 Ctrl+C:**

```
>> 阶段1: 自我对弈 (100局)...
   已完成: 10/100 局 (有效:10)

[用户按 Ctrl+C]

⚠️  检测到Ctrl+C，正在终止所有子进程...
请稍候（最多10秒）...
✓ 已停止对弈
提示: 已完成 10 局对弈，数据将被保存

对弈被中断

   统计结果...
   [中断] 完成了 10/100 局对弈
   统计: 红方0胜 黑方0胜 10和

>> 训练被中断，处理已收集的数据...

>> 阶段2: 训练神经网络...
   平均损失: 0.0102

>> 保存模型...
   模型已保存到: models/latest.pt
   总对局数: 205  (195 + 10)


训练被用户中断
训练已停止
```

**重新启动训练:**
```
已加载模型，已训练 205 局  ← 局数正确增加！
```

---

## 🔍 行为对比

| 行为 | 单进程 | 多进程（修复前） | 多进程（修复后） |
|------|--------|-----------------|-----------------|
| Ctrl+C 退出 | ✅ | ❌ (forrtl 错误) | ✅ |
| 无 CUDA 错误 | ✅ | ❌ | ✅ |
| 使用已完成数据 | ✅ | ❌ (数据丢失) | ✅ |
| total_games 增加 | ✅ | ❌ | ✅ |
| 训练网络 | ✅ | ❌ | ✅ |
| 保存模型 | ✅ | ✅ | ✅ |
| 停止训练循环 | ✅ | ✅ | ✅ |

**结论:** 多进程模式现在和单进程模式行为完全一致！

---

## 📝 修改文件清单

### 1. main.py
- 添加环境变量 `FOR_DISABLE_CONSOLE_CTRL_HANDLER` 禁用 MKL 信号处理
- 简化 KeyboardInterrupt 处理（模型在 trainer 中已保存）

### 2. self_play.py
- 新增 `InterruptedWithResults` 自定义异常类
- 修改 `_play_game_worker()` 设置环境变量和信号处理
- 修改 `parallel_self_play()` 将 GPU 张量转 CPU
- 修改 KeyboardInterrupt 处理，抛出自定义异常携带已完成数据

### 3. trainer.py
- 导入 `InterruptedWithResults` 异常类
- 修改 `collect_self_play_data()` 捕获自定义异常并处理已完成数据
- 修改 `train_loop()` 在 KeyboardInterrupt 时先训练再保存

---

## 💡 技术要点

### 1. Windows 多进程信号处理
- 主进程处理 SIGINT
- 子进程忽略 SIGINT (SIG_IGN)
- 使用环境变量禁用 MKL 信号拦截

### 2. PyTorch CUDA 多进程
- GPU 张量不能直接在进程间传递
- 必须先 `.cpu()` 转到 CPU
- 子进程中再 `.to(DEVICE)` 移到 GPU

### 3. 异常处理与数据保存
- 使用自定义异常携带数据
- 在多层 try-except 中逐层处理
- 确保数据处理完成后再抛出异常

### 4. 进程池管理
- `pool.terminate()` 强制终止
- `pool.join(timeout=10)` 等待最多10秒
- 用 try-except 包裹避免终止时的错误

---

## 🎉 成果

1. **多进程加速:** 保持 12倍 训练加速（4进程并行）
2. **优雅退出:** Ctrl+C 快速响应（10秒内），无需任务管理器
3. **数据安全:** 已完成的对局数据不丢失，全部用于训练
4. **状态一致:** 模型、局数、缓冲区全部正确保存
5. **用户体验:** 和单进程一样稳定可控，但速度快得多

---

## 🚀 使用建议

### 日常训练（推荐）

```python
# config.py
USE_MULTIPROCESSING = True  # 启用多进程加速
NUM_WORKERS = 4              # 根据CPU核心数调整
```

```bash
python main.py train

# 随时按 Ctrl+C 安全退出
# - 10秒内优雅退出
# - 已完成的数据会被保存
# - 局数正确增加
# - 下次继续训练
```

### 性能数据

- **单进程:** 34秒/局 (3倍加速，动态MCTS)
- **多进程:** 8.6秒/局 (12倍加速，4进程并行)
- **每轮时间 (100局):**
  - 原版: 2.8小时
  - 单进程优化: 0.94小时
  - 多进程优化: 0.24小时

---

## 📌 遗留问题

### 所有对局都是和棋
**现象:** `统计: 红方0胜 黑方0胜 10和`

**可能原因:**
1. 之前禁用了长捉检测，可能影响了胜负判断
2. 需要检查 `chess_env.py` 中的胜负判定逻辑
3. 可能是训练初期AI太弱，经常走到100步判和

**待排查:** 这个问题不影响训练流程，但会影响训练质量

---

## ✨ 总结

今天成功解决了 Windows 下多进程训练的所有 Ctrl+C 问题：
1. ✅ 消除了 MKL 错误
2. ✅ 修复了 CUDA 错误
3. ✅ 实现了数据保存
4. ✅ 保持了性能优势

**现在可以放心使用多进程模式进行长时间训练！** 🎯
